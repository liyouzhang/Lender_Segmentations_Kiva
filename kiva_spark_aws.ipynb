{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-17-168.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2f05d51cc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;51manaconda\u001b[0m@  df_ll.csv             \u001b[38;5;27mmetastore_db\u001b[0m/  vocs.json\r\n",
      "derby.log  kiva_spark_aws.ipynb  \u001b[38;5;27mscripts\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = spark.read.csv('kiva/df_ll.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+--------+\n",
      "|_c0|LOAN_ID|LENDERS_lst|lender_v|\n",
      "+---+-------+-----------+--------+\n",
      "|  0| 483693|     muc888|  970524|\n",
      "+---+-------+-----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ll.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|LOAN_ID|lender_v|\n",
      "+-------+--------+\n",
      "| 483693|  970524|\n",
      "+-------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ll.select('LOAN_ID','lender_v').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+--------+-----+\n",
      "|_c0|LOAN_ID|   LENDERS_lst|lender_v|value|\n",
      "+---+-------+--------------+--------+-----+\n",
      "|  0| 483693|        muc888|  970524|    1|\n",
      "|  1| 483693|       sam4326| 1153379|    1|\n",
      "|  2| 483693|   camaran3922|  187221|    1|\n",
      "|  3| 483693|   lachheb1865|  758772|    1|\n",
      "|  4| 483693|   rebecca3499| 1090160|    1|\n",
      "|  5| 483693| karlheinz4543|  687280|    1|\n",
      "|  6| 483693|       jerrydb|  588362|    1|\n",
      "|  7| 483693|     paula8951| 1043143|    1|\n",
      "|  8| 483693|          gmct|  460491|    1|\n",
      "|  9| 483693|      amra9383|   55499|    1|\n",
      "| 10| 483693|         r3922| 1070591|    1|\n",
      "| 11| 483693|     brian9451|  173534|    1|\n",
      "| 12| 483693|     shree8053| 1211636|    1|\n",
      "| 13| 483693|      alan5513|   20910|    1|\n",
      "| 14| 483693|     oisin3389| 1013325|    1|\n",
      "| 15| 483693|     helle8622|  493827|    1|\n",
      "| 16| 483693|        bo3186|  155244|    1|\n",
      "| 17| 483693|       ric8947| 1100676|    1|\n",
      "| 18| 483693|daniel98469874|  293820|    1|\n",
      "| 19| 483693|      nick9464|  997062|    1|\n",
      "+---+-------+--------------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_with_one = df_ll.withColumn(\"value\", lit(1))\n",
    "df_with_one.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = CoordinateMatrix(df_with_one.select('LOAN_ID','lender_v','value').rdd.map(lambda r: MatrixEntry(r.LOAN_ID, r.lender_v,r.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = df_with_one.select('LOAN_ID','lender_v','value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "|LOAN_ID|lender_v|value|\n",
      "+-------+--------+-----+\n",
      "| 483693|  970524|    1|\n",
      "| 483693| 1153379|    1|\n",
      "| 483693|  187221|    1|\n",
      "| 483693|  758772|    1|\n",
      "+-------+--------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ratings.randomSplit([1.0, 2.0], 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "|LOAN_ID|lender_v|value|\n",
      "+-------+--------+-----+\n",
      "|   1000|  308811|    1|\n",
      "|   1000|  579129|    1|\n",
      "+-------+--------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits[0].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9436011"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18858843"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(splits[1], rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits[0]..map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-22-2ac54626ffc6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-2ac54626ffc6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    predictions = model.predictAll(splits[0].rdd.map(lambda r: (r[0], r[1]))\u001b[0m\n\u001b[0m                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predictAll(splits[0].rdd.map(lambda r: (r[0], r[1]), r[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ALS.trainImplicit(ratings, rank, numIterations, alpha=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
